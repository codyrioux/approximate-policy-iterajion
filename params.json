{"name":"Approximate-policy-iterajion","tagline":"A reinforcement learning algorithm for exploring large Markov decision processes (MDP) implemented in Clojure.","body":"# approximate-policy-iterajion\r\n\r\nAn implementation of Approximate Policy Iteration (API) from the paper Lagoudakis et. al. 2003.\r\n\r\nThis is a reinforcement learning algorithm that exploits a classifier, in this case an svm, to select\r\nstate and action pairs in a large state space.\r\n\r\nThis algorithm approximates a policy for approximating solutions to very large Markov Decision Processes (MDP)\r\nin a parallel fashion. I plan on using it for part of a larger project, however this component itself is\r\nvery reusable so I factored it out into a library.\r\n\r\n## Usage\r\n\r\nAdd the following dependency to your `project.clj` file.\r\n\r\n```clojure\r\n[apprpoximate-policy-iterajion \"0.4.0\"]\r\n```\r\n\r\nAll of the following code can be found in `sample.clj`\r\n\r\nFor our toy problem we will be defining a simple problem in which an agent attempts to reach the number 10 using addition.\r\nIn this example a state will be a number, and an action will be a number as well (which gets added to the state).\r\n\r\n```clojure\r\n(def goal 10)\r\n```\r\n\r\nFirst we need a generative model that will take state and action pairs, and return the new state `sprime` and a reward `r`.\r\nTo generate sprime we add `s + a` and to generate a reward we compute `1 / (|goal - (s + a)\\) + 0.01)`\r\n\r\n```clojure\r\n(defn m\r\n  \"States and actions are added.\"\r\n  [s a]\r\n  (+ s a))\r\n```\r\n\r\nNow a reward function.\r\n\r\n```clojure\r\n(defn reward\r\n  [s]\r\n  (/ 1 (+ 0.01 (Math/abs (- goal s)))))\r\n```\r\n\r\nNow we need a function to generate a bunch of starting states. For our problem we will start at every number from\r\n0 to 20.\r\n\r\n```clojure\r\n(defn dp\r\n  \"0 to goal * 2 for starting states\"\r\n  []\r\n  (range 0 (* goal 2)))\r\n```\r\n\r\nNow we require a function `sp` that generates actions for a given state. In this example actions available are the same\r\nno matter the state, however in a real world problem actions will vary by state. In this case we will allow the user to\r\nadd any number between `-(goal / 2) and (goal / 2)`\r\n\r\n```clojure\r\n(defn sp\r\n  \"Can add or subtract up to half of the goal.\"\r\n  [s]\r\n  (range (* -1 (/ goal 2)) (/ goal 2)))\r\n```\r\n\r\nLastly we require a feature extraction function in order to teach our learner. approximate-policy-iterajion uses svm-clj\r\nunder the hood so our features are maps of increasing numbers 1..n to the feature value.\r\n\r\n```clojure\r\n(defn features\r\n  [s a]\r\n  {1 s\r\n   2 (- goal s)\r\n   3 (if (pos? s) 1 0)\r\n   4 (if (pos? (- goal s)) 1 0)\r\n   5 (if (> goal s) 1 0)\r\n   6 a\r\n   })\r\n```\r\n\r\nNow that we have defined m, dp, sp, and features we can run approximate policy iteration with 300 rollouts per state,\r\n and a trajectory length of 5 per rollout using a discount factor of 0.99.\r\n\r\n```clojure\r\n(use 'approximate-policy-iterajion.core)\r\n\r\n(def my-policy (api/api m reward dp sp 0.99 300 10 features \"sample\" :kernel-type (:rbf api/kernel-types))))\r\n\r\n; We get some output from the underlying svm implementation\r\n\r\n; Now lets ask the policy for an action given our state s\r\n\r\n(my-policy 0)\r\n;=> 4\r\n(my-policy 4)\r\n;=> 4 \r\n(my-policy 8)\r\n;=> 2\r\n(my-policy 10)\r\n;=> 0\r\n```\r\n\r\nAll of this code is available in `sample.clj` and can be run simply by calling:\r\n\r\n```clojure\r\n(use 'approximate-policy-iterajion.sample :reload-all)\r\n(def my-policy (create-api-policy 300 10))\r\n(my-policy 0)\r\n;=> 4\r\n(my-policy 4)\r\n;=> 4 \r\n(my-policy 8)\r\n;=> 2\r\n(my-policy 10)\r\n;=> 0\r\n```\r\n\r\nNow take this and build your own reinforcement learning solutions to problems. :D\r\n\r\n## Changelog\r\n\r\n### 0.4.0\r\nSimplified the API function signature, policy is no longer a parameter. Implements a proper greedy (on estimated value)\r\nreturned policy.\r\n\r\n### 0.3.11\r\nAnother attempt at fixing the divide by zero bug occuring in the t-test that determines\r\nsignificance.\r\n\r\n### 0.3.10\r\n16 agents are now used for parallelism in the application.\r\n\r\n### 0.3.9\r\nThe bug supposedly fixed in 0.3.7 appears to still exist, and this is another attempt at fixing\r\nsaid bug.\r\n\r\n### 0.3.8\r\nMoved the default policy to one that is random during training and greedy on the estimated (rollout)\r\nvalues during testing.\r\n\r\n### 0.3.7\r\nFixed a bug in which an exception was thrown if qpi contained only one state-action pair.\r\n\r\n### 0.3.6\r\nAdded an id parameter to the `api` function. This allows the run to identify itself and persist\r\nand load its training set in case of interruption. Useful for EC2 spot instance computation.\r\n\r\n### 0.3.5\r\nAdded a branching factor parameter to the `api` function. This allows you to chunk the dataset into the\r\nspecified number of pieces for parallel processing. In experimentation the default pmap settings did not\r\nwork well. Setting the number to the number of processors in the machine proved much more useful.\r\n\r\n### 0.3.4\r\n\r\nRemoved the pmap from the rollout function. It appears as though any attempt at using the svm model in parallel creates\r\na resource deadlock. I'll need to explore classifiers in the future that will work for this purpose.\r\n\r\n### 0.3.3\r\nThe parameter function `dp` is now provided with `states-1` the set of states used in the last iteration and `pi` the policy.\r\nThe intention is to allow people to guide their state generation using the policy in the event that the state space is very large.\r\n\r\n\r\n## Todo\r\n* Unit Tests\r\n* Explore alternate classifiers\r\n\r\n## License\r\n\r\nCopyright Â© 2013 Cody Rioux\r\nDistributed under the Eclipse Public License, the same as Clojure.\r\n","google":"UA-3827325-2","note":"Don't delete this file! It's used internally to help with page regeneration."}