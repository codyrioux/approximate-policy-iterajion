<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Approximate-policy-iterajion by codyrioux</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>Approximate-policy-iterajion</h1>
        <p>A reinforcement learning algorithm for exploring large Markov decision processes (MDP) implemented in Clojure.</p>

        <p class="view"><a href="https://github.com/codyrioux/approximate-policy-iterajion">View the Project on GitHub <small>codyrioux/approximate-policy-iterajion</small></a></p>


        <ul>
          <li><a href="https://github.com/codyrioux/approximate-policy-iterajion/zipball/master">Download <strong>ZIP File</strong></a></li>
          <li><a href="https://github.com/codyrioux/approximate-policy-iterajion/tarball/master">Download <strong>TAR Ball</strong></a></li>
          <li><a href="https://github.com/codyrioux/approximate-policy-iterajion">View On <strong>GitHub</strong></a></li>
        </ul>
      </header>
      <section>
        <h1>
<a name="approximate-policy-iterajion" class="anchor" href="#approximate-policy-iterajion"><span class="octicon octicon-link"></span></a>approximate-policy-iterajion</h1>

<p>An implementation of Approximate Policy Iteration (API) from the paper Lagoudakis et. al. 2003.</p>

<p>This is a reinforcement learning algorithm that exploits a classifier, in this case an svm, to select
state and action pairs in a large state space.</p>

<p>This algorithm approximates a policy for approximating solutions to very large Markov Decision Processes (MDP)
in a parallel fashion. I plan on using it for part of a larger project, however this component itself is
very reusable so I factored it out into a library.</p>

<h2>
<a name="usage" class="anchor" href="#usage"><span class="octicon octicon-link"></span></a>Usage</h2>

<p>Add the following dependency to your <code>project.clj</code> file.</p>

<div class="highlight"><pre><span class="p">[</span><span class="nv">apprpoximate-policy-iterajion</span> <span class="s">"0.4.4"</span><span class="p">]</span>
</pre></div>

<p>All of the following code can be found in <code>sample.clj</code></p>

<p>For our toy problem we will be defining a simple problem in which an agent attempts to reach the number 10 using addition.
In this example a state will be a number, and an action will be a number as well (which gets added to the state).</p>

<div class="highlight"><pre><span class="p">(</span><span class="k">def </span><span class="nv">goal</span> <span class="mi">10</span><span class="p">)</span>
</pre></div>

<p>First we need a generative model that will take state and action pairs, and return the new state <code>sprime</code> and a reward <code>r</code>.
To generate sprime we add <code>s + a</code> and to generate a reward we compute <code>1 / (|goal - (s + a)\) + 0.01)</code></p>

<div class="highlight"><pre><span class="p">(</span><span class="kd">defn </span><span class="nv">m</span>
  <span class="s">"States and actions are added."</span>
  <span class="p">[</span><span class="nv">s</span> <span class="nv">a</span><span class="p">]</span>
  <span class="p">(</span><span class="nf">cond</span>
    <span class="p">(</span><span class="nb">nil? </span><span class="nv">a</span><span class="p">)</span> <span class="nv">s</span>
    <span class="ss">:else</span> <span class="p">(</span><span class="nb">+ </span><span class="nv">s</span> <span class="nv">a</span><span class="p">))</span>
</pre></div>

<p>Now a reward function.</p>

<div class="highlight"><pre><span class="p">(</span><span class="kd">defn </span><span class="nv">reward</span>
  <span class="p">[</span><span class="nv">s</span><span class="p">]</span>
  <span class="p">(</span><span class="nb">/ </span><span class="mi">1</span> <span class="p">(</span><span class="nb">+ </span><span class="mf">0.01</span> <span class="p">(</span><span class="nf">Math/abs</span> <span class="p">(</span><span class="nb">- </span><span class="nv">goal</span> <span class="nv">s</span><span class="p">)))))</span>
</pre></div>

<p>Now we need a function to generate a bunch of starting states. For our problem we will start at every number from
0 to 20.</p>

<div class="highlight"><pre><span class="p">(</span><span class="kd">defn </span><span class="nv">dp</span>
  <span class="s">"0 to goal * 2 for starting states"</span>
  <span class="p">[</span><span class="nv">states-1</span> <span class="nv">pi</span><span class="p">]</span>
  <span class="p">(</span><span class="nb">range </span><span class="mi">0</span> <span class="p">(</span><span class="nb">* </span><span class="nv">goal</span> <span class="mi">2</span><span class="p">)))</span>
</pre></div>

<p>Now we require a function <code>sp</code> that generates actions for a given state. In this example actions available are the same
no matter the state, however in a real world problem actions will vary by state. In this case we will allow the user to
add any number between <code>-(goal / 2) and (goal / 2)</code></p>

<div class="highlight"><pre><span class="p">(</span><span class="kd">defn </span><span class="nv">sp</span>
  <span class="s">"Can add or subtract up to half of the goal."</span>
  <span class="p">[</span><span class="nv">s</span><span class="p">]</span>
  <span class="p">(</span><span class="nf">cond</span>
   <span class="p">(</span><span class="nb">= </span><span class="nv">goal</span> <span class="nv">s</span><span class="p">)</span> <span class="p">[]</span>
   <span class="ss">:else</span> <span class="p">(</span><span class="nb">range </span><span class="p">(</span><span class="nb">* </span><span class="mi">-1</span> <span class="p">(</span><span class="nb">/ </span><span class="nv">goal</span> <span class="mi">2</span><span class="p">))</span> <span class="p">(</span><span class="nb">/ </span><span class="nv">goal</span> <span class="mi">2</span><span class="p">))))</span>
</pre></div>

<p>Lastly we require a feature extraction function in order to teach our learner. approximate-policy-iterajion uses svm-clj
under the hood so our features are maps of increasing numbers 1..n to the feature value.</p>

<div class="highlight"><pre><span class="p">(</span><span class="kd">defn </span><span class="nv">features</span>
  <span class="p">[</span><span class="nv">s</span> <span class="nv">a</span><span class="p">]</span>
  <span class="p">{</span><span class="mi">1</span> <span class="nv">s</span>
   <span class="mi">2</span> <span class="p">(</span><span class="nb">- </span><span class="nv">goal</span> <span class="nv">s</span><span class="p">)</span>
   <span class="mi">3</span> <span class="p">(</span><span class="k">if </span><span class="p">(</span><span class="nb">pos? </span><span class="nv">s</span><span class="p">)</span> <span class="mi">1</span> <span class="mi">0</span><span class="p">)</span>
   <span class="mi">4</span> <span class="p">(</span><span class="k">if </span><span class="p">(</span><span class="nb">pos? </span><span class="p">(</span><span class="nb">- </span><span class="nv">goal</span> <span class="nv">s</span><span class="p">))</span> <span class="mi">1</span> <span class="mi">0</span><span class="p">)</span>
   <span class="mi">5</span> <span class="p">(</span><span class="k">if </span><span class="p">(</span><span class="nb">&gt; </span><span class="nv">goal</span> <span class="nv">s</span><span class="p">)</span> <span class="mi">1</span> <span class="mi">0</span><span class="p">)</span>
   <span class="mi">6</span> <span class="nv">a</span>
   <span class="p">})</span>
</pre></div>

<p>Now that we have defined m, dp, sp, and features we can run approximate policy iteration with 300 rollouts per state,
 and a trajectory length of 10 per rollout using a discount factor of 0.99.</p>

<div class="highlight"><pre><span class="p">(</span><span class="nf">use</span> <span class="ss">'approximate-policy-iterajion.core</span><span class="p">)</span>

<span class="p">(</span><span class="k">def </span><span class="nv">my-policy</span> <span class="p">(</span><span class="nf">api/api</span> <span class="nv">m</span> <span class="nv">reward</span> <span class="nv">dp</span> <span class="nv">sp</span> <span class="mf">0.99</span> <span class="mi">300</span> <span class="mi">10</span> <span class="nv">features</span> <span class="s">"sample"</span> <span class="mi">5</span> <span class="ss">:kernel-type</span> <span class="p">(</span><span class="ss">:rbf</span> <span class="nv">api/kernel-types</span><span class="p">))))</span>

<span class="c1">; We get some output from the underlying svm implementation</span>

<span class="c1">; Now lets ask the policy for an action given our state s</span>

<span class="p">(</span><span class="nf">my-policy</span> <span class="mi">0</span><span class="p">)</span>
<span class="c1">;=&gt; 4</span>
<span class="p">(</span><span class="nf">my-policy</span> <span class="mi">4</span><span class="p">)</span>
<span class="c1">;=&gt; 4 </span>
<span class="p">(</span><span class="nf">my-policy</span> <span class="mi">8</span><span class="p">)</span>
<span class="c1">;=&gt; 2</span>
<span class="p">(</span><span class="nf">my-policy</span> <span class="mi">10</span><span class="p">)</span>
<span class="c1">;=&gt; nil</span>
</pre></div>

<p>All of this code is available in <code>sample.clj</code> and can be run simply by calling:</p>

<div class="highlight"><pre><span class="p">(</span><span class="nf">use</span> <span class="ss">'approximate-policy-iterajion.sample</span> <span class="ss">:reload-all</span><span class="p">)</span>
<span class="p">(</span><span class="k">def </span><span class="nv">my-policy</span> <span class="p">(</span><span class="nf">create-api-policy</span> <span class="mi">300</span> <span class="mi">10</span><span class="p">))</span>
<span class="p">(</span><span class="nf">my-policy</span> <span class="mi">0</span><span class="p">)</span>
<span class="c1">;=&gt; 4</span>
<span class="p">(</span><span class="nf">my-policy</span> <span class="mi">4</span><span class="p">)</span>
<span class="c1">;=&gt; 4 </span>
<span class="p">(</span><span class="nf">my-policy</span> <span class="mi">8</span><span class="p">)</span>
<span class="c1">;=&gt; 2</span>
<span class="p">(</span><span class="nf">my-policy</span> <span class="mi">10</span><span class="p">)</span>
<span class="c1">;=&gt; nil</span>
</pre></div>

<p>Now take this and build your own reinforcement learning solutions to problems. :D</p>

<h2>
<a name="changelog" class="anchor" href="#changelog"><span class="octicon octicon-link"></span></a>Changelog</h2>

<h3>
<a name="044" class="anchor" href="#044"><span class="octicon octicon-link"></span></a>0.4.4</h3>

<p>Altered the generated policy so that operations are performed in parallel. The reasoning here
is that should the policy encounter a state it has not seen and need to evaluate many actions
the performance gain is large, whereas if a small number of actions are up for evaluation
the performance loss will be minimal.</p>

<h3>
<a name="043" class="anchor" href="#043"><span class="octicon octicon-link"></span></a>0.4.3</h3>

<p>Altered the code base so that situations in which no action exist can be handled. In this case
the policy functions return nil. Therefore your state generator <code>sp</code> can return an empty list
and your generative model <code>m</code> should be able to handle nil in place of action.</p>

<h3>
<a name="042" class="anchor" href="#042"><span class="octicon octicon-link"></span></a>0.4.2</h3>

<p>Added a maximum iterations (mi) parameter to api. Allows the user to constrain the run
time of the learner.</p>

<h3>
<a name="041" class="anchor" href="#041"><span class="octicon octicon-link"></span></a>0.4.1</h3>

<p>The deployment to Clojars failed for 0.4.0 so I needed to push a new version.</p>

<h3>
<a name="040" class="anchor" href="#040"><span class="octicon octicon-link"></span></a>0.4.0</h3>

<p>Simplified the API function signature, policy is no longer a parameter. Implements a proper greedy (on estimated value)
returned policy.</p>

<h3>
<a name="0311" class="anchor" href="#0311"><span class="octicon octicon-link"></span></a>0.3.11</h3>

<p>Another attempt at fixing the divide by zero bug occuring in the t-test that determines
significance.</p>

<h3>
<a name="0310" class="anchor" href="#0310"><span class="octicon octicon-link"></span></a>0.3.10</h3>

<p>16 agents are now used for parallelism in the application.</p>

<h3>
<a name="039" class="anchor" href="#039"><span class="octicon octicon-link"></span></a>0.3.9</h3>

<p>The bug supposedly fixed in 0.3.7 appears to still exist, and this is another attempt at fixing
said bug.</p>

<h3>
<a name="038" class="anchor" href="#038"><span class="octicon octicon-link"></span></a>0.3.8</h3>

<p>Moved the default policy to one that is random during training and greedy on the estimated (rollout)
values during testing.</p>

<h3>
<a name="037" class="anchor" href="#037"><span class="octicon octicon-link"></span></a>0.3.7</h3>

<p>Fixed a bug in which an exception was thrown if qpi contained only one state-action pair.</p>

<h3>
<a name="036" class="anchor" href="#036"><span class="octicon octicon-link"></span></a>0.3.6</h3>

<p>Added an id parameter to the <code>api</code> function. This allows the run to identify itself and persist
and load its training set in case of interruption. Useful for EC2 spot instance computation.</p>

<h3>
<a name="035" class="anchor" href="#035"><span class="octicon octicon-link"></span></a>0.3.5</h3>

<p>Added a branching factor parameter to the <code>api</code> function. This allows you to chunk the dataset into the
specified number of pieces for parallel processing. In experimentation the default pmap settings did not
work well. Setting the number to the number of processors in the machine proved much more useful.</p>

<h3>
<a name="034" class="anchor" href="#034"><span class="octicon octicon-link"></span></a>0.3.4</h3>

<p>Removed the pmap from the rollout function. It appears as though any attempt at using the svm model in parallel creates
a resource deadlock. I'll need to explore classifiers in the future that will work for this purpose.</p>

<h3>
<a name="033" class="anchor" href="#033"><span class="octicon octicon-link"></span></a>0.3.3</h3>

<p>The parameter function <code>dp</code> is now provided with <code>states-1</code> the set of states used in the last iteration and <code>pi</code> the policy.
The intention is to allow people to guide their state generation using the policy in the event that the state space is very large.</p>

<h2>
<a name="todo" class="anchor" href="#todo"><span class="octicon octicon-link"></span></a>Todo</h2>

<ul>
<li>Unit Tests</li>
<li>Explore alternate classifiers</li>
</ul><h2>
<a name="license" class="anchor" href="#license"><span class="octicon octicon-link"></span></a>License</h2>

<p>Copyright © 2013 Cody Rioux
Distributed under the Eclipse Public License, the same as Clojure.</p>
      </section>
      <footer>
        <p>This project is maintained by <a href="https://github.com/codyrioux">codyrioux</a></p>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
              <script type="text/javascript">
            var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
            document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
          </script>
          <script type="text/javascript">
            try {
              var pageTracker = _gat._getTracker("UA-3827325-2");
            pageTracker._trackPageview();
            } catch(err) {}
          </script>

  </body>
</html>