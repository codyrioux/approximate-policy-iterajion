(ns approximate-policy-iterajion.core
  "For each state generated by dp, and each action for that state generated by sp
   performs a rollout, stores each estimated value in qpi. Rollout: [sprime value]
   Then takes the maximum estimated value and stores it in a*.
   Then extracts the positive, negative training examples, unions them with the current ones.
   Terminates when the policy converges, ie. the training set does not change.

   The initial policy is to randomly select an action, this is in effect only on the first iteration
   before training examples are generated.

   Call api with specified parameters to generate a policy function."
  (:require [incanter.stats :as stats])
  (:use [clojure.set]
        [svm.core]))

(defn statistically-significant?
  "Determines if the provided target-sample is statistically significant compared to all samples.  Arguments:
   p-threshold    : The p value threshold for the t-test. ex. 0.05 or 0.01
   score          : A function returning the score of a sample. Note if precomputed in a collection/hash
   this function could be first or :score for example.
   samples        : A collection of all of the samples.
   target-sample  : The sample for which we would like to determine statistically significant difference.

   Examples:
   (statistically-significant? identity 0.05 (range 1 10) 15)"
  [score p-threshold samples target-sample]
  (>= p-threshold (:p-value (stats/t-test 
                              (map score samples) 
                              :mu (score target-sample)))))

(defn policy
  "Executes the policy on the corresponding state and determines a proper action.
   Generates all possible actions using sp, then maps these actions to their state using m
   It then  classifies each as either positive 1.0 or negative -1.0, placing them in a tuple.

   Note: This is an example policy and you are encouraged to roll your own.

   Arguments: 
   feature-extractor : A feature extractor function that takes a state and returns a set of features for the learner.
   rw                : Reward function that takes a state and returns a reward.
   sp                : A function that takes a state and returns all possible actions.
   m                 : A generative model that takes (s, a) and returns a new state.
   model             : An svm-clj model trained to implement the policy.
   state             : The current state to use as a base for the next action.

   The algorithm will take all positive samples and attempt to maximize reward.
   If no actions are classified it attempts to maximize reward over all actions.

   Returns: The next action as decided by the policy,
   randomly selected if there are multiple positive."
  [feature-extractor rw model sp m state]
  (let [actions (filter #(= 1.0 (predict model (feature-extractor (m state %1)))) (sp state))
        actions (if (> (count actions) 0) actions (sp state))
        rw-actions (zipmap (map #( rw (m state %)) actions) actions)]
    (rw-actions (reduce max (keys rw-actions)))))

(defn- rollout
  "This function estimates the value of a state-action pair using rollouts. The underlying concept
   is that the state space for our Markov Decision Process (MDP) is too large to compute exactly.

   Arguments:
   m  : Generative model a function that takes s, a and a seed integer.
   s  : A state, paired with the action.
   a  : An action, paired with the state, represents the initial action for this trajectory.
   y  : Discount factor. 0 < y <= 1
   pi : A policy.
   k  : Number of trajectories.
   t  : The length of each trajectory.

   Returns:
   A tuple of the form [new-state approximated-value]"
  [m, rw, s, a, y, pi, k, t]
  [(m s a) (* (/ 1 k) (reduce + (pmap (fn [_] (let [sprime (m s a) r (rw sprime)]
                                                (loop [s sprime, t t, qk r, y y]
                                                  (cond 
                                                    (= 0 t) qk
                                                    :else (let [sprime (m s (pi s)) r (rw sprime)]
                                                            (recur sprime (- t 1) (+ qk (* (Math/pow y t) r)) y)))))) (range 0 k))))])

(defn- get-positive-samples
  "Takes a series of rollout scores and returns a set containing a single positive training example.

   samples: A seq of rollout examples where the second element is the score.
   a*:      The optimal approximated value of the set."
  [feature-extractor samples a*]
  (let [target-sample (first (filter #(= a* (second %1)) samples))
        significant (statistically-significant? second 0.05 samples target-sample)]
    (if significant [ [1.0 (feature-extractor (first target-sample))]] [])))

(defn- get-negative-samples
  "Takes a series of rollout scores and returns a set containing all the negative training examples.
   Filtering below the mean ensures that regardless of wether or not a* is significant only negative
   significant examples are returned."
  [feature-extractor samples]
  (let [sample-mean (/ (reduce + (map second samples)) (count samples))]
    (->>
      (filter #(> sample-mean (second %1)) samples)
      (filter #(statistically-significant? second 0.05 samples %1))
      (map #(vec [-1.0 (feature-extractor (first %1))])))))

(defn api
  "The primary function for approximate policy iteration.

   Arguments:
   m  : Generative model.
   rw : Reward function, takes a state and returns a reward value.
   dp : A source of rollout states, a fn that returns a set of states. (dp)
   sp : A source of available actions for a state, an fn that takes a state and returns actions.
   (sp state)
   y  : Discount factor. [0, 1
   pi : A policy function that takes a model and state, returns an action.
   k  : The number of trajectories to compute on each rollout.
   t  : The length of each trajectory.
   fe : A function that extracts features from a state. {1 feature1, 2 feature2, ...}

   Returns: A function pi that takes state and returns an action."
  [m rw dp sp y pi0 k t fe]
  (loop [pi #(rand-nth (sp %)) ts [] tsi-1 nil]
    (cond
      (= tsi-1 ts) pi
      :else (let [qpi (apply concat (for [s (dp)] (for [a (sp s)] (rollout m rw s a y pi k t)))) 
                  a* (apply max (map second qpi))
                  next-ts (concat (get-positive-samples fe qpi a*) ( get-negative-samples fe qpi))]
              (recur (partial pi0 (train-model next-ts) sp m) next-ts ts)))))
